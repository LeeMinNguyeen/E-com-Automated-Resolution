{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a0a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project\\E-com-Automated-Resolution\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DistilBertPreTrainedModel, DistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a534f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "TRAIN_DATA_CSV = '../data/nlu_training_data.csv'\n",
    "MODEL_SAVE_PATH = '../model/nlu_model'\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d7324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Custom Multi-Task Model ---\n",
    "# This is the core of the DL model\n",
    "# It has a shared DistilBERT base and two separate output layers.\n",
    "class MultiTaskDistilBert(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, num_intent_labels, num_sentiment_labels):\n",
    "        super().__init__(config)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        \n",
    "        # Classifier head for Intent\n",
    "        self.intent_classifier = nn.Linear(config.dim, num_intent_labels)\n",
    "        # Classifier head for Sentiment\n",
    "        self.sentiment_classifier = nn.Linear(config.dim, num_sentiment_labels)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,  # Not used directly here, handled in training loop\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Get the last hidden state from the base DistilBERT model\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Get the [CLS] token representation (for classification)\n",
    "        pooled_output = distilbert_output[0][:, 0]  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Pass the output through each specific classifier head\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "\n",
    "        # Return the logits for both tasks\n",
    "        return (intent_logits, sentiment_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc03974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Create a Custom PyTorch Dataset ---\n",
    "class NLU_Dataset(Dataset):\n",
    "    def __init__(self, texts, intent_labels, sentiment_labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.intent_labels = intent_labels\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'intent_label': torch.tensor(self.intent_labels[idx], dtype=torch.long),\n",
    "            'sentiment_label': torch.tensor(self.sentiment_labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d545dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Helper Functions ---\n",
    "def load_data(csv_path):\n",
    "    \"\"\"Loads and preprocesses data, creating label mappings.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Training data file '{csv_path}' not found.\")\n",
    "        print(\"Please run 'label_data.py' first to create it.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Create mappings for our labels to convert them to numbers\n",
    "    intent_labels = {label: i for i, label in enumerate(df['intent'].unique())}\n",
    "    sentiment_labels = {label: i for i, label in enumerate(df['sentiment'].unique())}\n",
    "    \n",
    "    # Save the mappings so our agent can use them later\n",
    "    label_info = {\n",
    "        'intent_labels': intent_labels,\n",
    "        'sentiment_labels': sentiment_labels\n",
    "    }\n",
    "    \n",
    "    # Apply the mappings to the dataframe\n",
    "    df['intent_label'] = df['intent'].map(intent_labels)\n",
    "    df['sentiment_label'] = df['sentiment'].map(sentiment_labels)\n",
    "    \n",
    "    return df, label_info, len(intent_labels), len(sentiment_labels)\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    \"\"\"Calculates accuracy and F1 score.\"\"\"\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc56ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Main Training Function ---\n",
    "def train():\n",
    "    print(\"Step 1: Loading and preprocessing data...\")\n",
    "    df, label_info, num_intent_labels, num_sentiment_labels = load_data(TRAIN_DATA_CSV)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df['intent'])\n",
    "    \n",
    "    print(f\"Training samples: {len(df_train)}, Validation samples: {len(df_val)}\")\n",
    "    print(f\"Found {num_intent_labels} intents and {num_sentiment_labels} sentiments.\")\n",
    "\n",
    "    print(\"\\nStep 2: Initializing tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load our custom model\n",
    "    model = MultiTaskDistilBert.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_intent_labels=num_intent_labels,\n",
    "        num_sentiment_labels=num_sentiment_labels\n",
    "    )\n",
    "\n",
    "    # Set up datasets and dataloaders\n",
    "    train_dataset = NLU_Dataset(\n",
    "        texts=df_train.text.to_numpy(),\n",
    "        intent_labels=df_train.intent_label.to_numpy(),\n",
    "        sentiment_labels=df_train.sentiment_label.to_numpy(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    val_dataset = NLU_Dataset(\n",
    "        texts=df_val.text.to_numpy(),\n",
    "        intent_labels=df_val.intent_label.to_numpy(),\n",
    "        sentiment_labels=df_val.sentiment_label.to_numpy(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Set up optimizer and loss functions\n",
    "    # Force GPU usage - raise error if not available\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"GPU (CUDA) is not available. This training requires a GPU.\")\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # We use CrossEntropyLoss for both classification tasks\n",
    "    intent_loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    sentiment_loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    print(f\"\\nStep 3: Starting training for {NUM_EPOCHS} epochs on {device}...\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            intent_labels = batch['intent_label'].to(device)\n",
    "            sentiment_labels = batch['sentiment_label'].to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            intent_logits, sentiment_logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate combined loss\n",
    "            loss_intent = intent_loss_fn(intent_logits, intent_labels)\n",
    "            loss_sentiment = sentiment_loss_fn(sentiment_logits, sentiment_labels)\n",
    "            total_loss = loss_intent + loss_sentiment # We can weigh these if one is more important\n",
    "            \n",
    "            total_train_loss += total_loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_intent_preds, all_intent_labels = [], []\n",
    "        all_sentiment_preds, all_sentiment_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                intent_labels = batch['intent_label'].to(device)\n",
    "                sentiment_labels = batch['sentiment_label'].to(device)\n",
    "\n",
    "                intent_logits, sentiment_logits = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                loss_intent = intent_loss_fn(intent_logits, intent_labels)\n",
    "                loss_sentiment = sentiment_loss_fn(sentiment_logits, sentiment_labels)\n",
    "                total_loss = loss_intent + loss_sentiment\n",
    "                \n",
    "                total_val_loss += total_loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                intent_preds = torch.argmax(intent_logits, dim=1).cpu().numpy()\n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                all_intent_preds.extend(intent_preds)\n",
    "                all_intent_labels.extend(intent_labels.cpu().numpy())\n",
    "                all_sentiment_preds.extend(sentiment_preds)\n",
    "                all_sentiment_labels.extend(sentiment_labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        intent_metrics = compute_metrics(all_intent_preds, all_intent_labels)\n",
    "        sentiment_metrics = compute_metrics(all_sentiment_preds, all_sentiment_labels)\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Intent Metrics:     {intent_metrics}\")\n",
    "        print(f\"Sentiment Metrics:  {sentiment_metrics}\")\n",
    "\n",
    "    print(\"\\nStep 4: Training complete. Saving model...\")\n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained(MODEL_SAVE_PATH)\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    # Save the label info\n",
    "    with open(os.path.join(MODEL_SAVE_PATH, 'label_info.json'), 'w') as f:\n",
    "        json.dump(label_info, f, indent=4)\n",
    "        \n",
    "    print(f\"Model, tokenizer, and label info saved to '{MODEL_SAVE_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preprocessing data...\n",
      "Training samples: 80000, Validation samples: 20000\n",
      "Found 5 intents and 3 sentiments.\n",
      "\n",
      "Step 2: Initializing tokenizer and model...\n",
      "Training samples: 80000, Validation samples: 20000\n",
      "Found 5 intents and 3 sentiments.\n",
      "\n",
      "Step 2: Initializing tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskDistilBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['intent_classifier.bias', 'intent_classifier.weight', 'sentiment_classifier.bias', 'sentiment_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory: 4.00 GB\n",
      "\n",
      "Step 3: Starting training for 4 epochs on cuda...\n",
      "\n",
      "Step 3: Starting training for 4 epochs on cuda...\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
